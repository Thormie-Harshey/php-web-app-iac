# Define the pipeline stages in order of execution.
stages:
  - bootstrap
  - setup
  - validate
  - plan
  - apply
  - destroy

# Use a standard Terraform Docker image for all jobs.
image:
  name: hashicorp/terraform:1.10
  entrypoint: ["/bin/sh", "-c"]

# Define a hidden job with common setup for AWS authentication.
.aws_setup:
  id_tokens:
    GITLAB_OIDC_TOKEN:
      aud: https://gitlab.com
  before_script:
    - apk --no-cache add curl python3 py3-pip
    - pip3 install --no-cache-dir awscli --break-system-packages
    - >
      export $(printf "AWS_ACCESS_KEY_ID=%s AWS_SECRET_ACCESS_KEY=%s AWS_SESSION_TOKEN=%s"
      $(aws sts assume-role-with-web-identity
      --role-arn ${ROLE_ARN}
      --role-session-name "GitLabRunner-${CI_PROJECT_ID}-${CI_PIPELINE_ID}"
      --web-identity-token ${GITLAB_OIDC_TOKEN}
      --duration-seconds 3600
      --query 'Credentials.[AccessKeyId,SecretAccessKey,SessionToken]'
      --output text))
    - aws configure set region us-east-1

# This job creates the S3 bucket and DynamoDB table for Terraform state.
# It is manual and should be run only once to set up the backend.
bootstrap:
  stage: bootstrap
  extends: .aws_setup
  script:
    - echo "Checking if S3 bucket ${TF_STATE_BUCKET} exists..."
    - >
      if aws s3api head-bucket --bucket ${TF_STATE_BUCKET} 2>/dev/null; then
        echo "S3 bucket already exists, skipping creation."
      else
        echo "S3 bucket does not exist, creating it..."
        aws s3api create-bucket --bucket ${TF_STATE_BUCKET}
        echo "Bucket created. Enabling versioning..."
        aws s3api put-bucket-versioning --bucket ${TF_STATE_BUCKET} --versioning-configuration Status=Enabled
      fi
    - echo "Checking if DynamoDB table ${TF_STATE_TABLE} exists..."
    - >
      if aws dynamodb describe-table --table-name ${TF_STATE_TABLE} 2>/dev/null; then
        echo "DynamoDB table already exists, skipping creation."
      else
        echo "DynamoDB table does not exist, creating it..."
        aws dynamodb create-table \
          --table-name ${TF_STATE_TABLE} \
          --attribute-definitions AttributeName=LockID,AttributeType=S \
          --key-schema AttributeName=LockID,KeyType=HASH \
          --billing-mode PAY_PER_REQUEST
      fi
  when: manual
  allow_failure: true

# This is a one-time manual job. It initializes Terraform and imports
# the backend resources (S3 bucket and DynamoDB table) into the state file.
# It saves the state file as an artifact for subsequent jobs.
setup:
  stage: setup
  extends: .aws_setup
  script:
    - echo "Initializing Terraform with backend configuration..."
    - terraform init -backend-config="bucket=${TF_STATE_BUCKET}" -backend-config="dynamodb_table=${TF_STATE_TABLE}"
    
    - echo "Importing S3 state bucket and DynamoDB lock table..."
    - terraform import module.s3.aws_s3_bucket.state_bucket "${TF_STATE_BUCKET}"
    - terraform import module.dynamodb_state_lock.aws_dynamodb_table.state_lock_table "${TF_STATE_TABLE}"
    
  artifacts:
    paths:
      - .terraform/
      - .terraform.lock.hcl
  when: manual
  dependencies: []

# This job validates the Terraform code for syntax and configuration.
# It depends on the 'setup' job to get the initialized state and providers.
validate:
  stage: validate
  extends: .aws_setup
  script:
    - terraform fmt -check -recursive
    - terraform validate
  dependencies:
    - setup

# This job creates a Terraform plan and saves it as an artifact.
# It depends on 'setup' and uses a re-init to ensure it has the latest state.
plan:
  stage: plan
  extends: .aws_setup
  script:
    - terraform init -reconfigure -backend-config="bucket=${TF_STATE_BUCKET}" -backend-config="dynamodb_table=${TF_STATE_TABLE}"
    - terraform plan -out="planfile"
    - terraform show -no-color planfile > planfile.txt
  artifacts:
    paths:
      - planfile
      - .terraform.lock.hcl
    reports:
      terraform: planfile
  dependencies:
    - setup

# This job applies the plan to create the infrastructure.
# It is manual to prevent accidental changes and depends on 'plan'.
apply:
  stage: apply
  extends: .aws_setup
  script:
    - terraform apply -input=false "planfile"
    - terraform output > output.txt
  artifacts:
    paths:
      - output.txt
    reports:
      terraform: output.txt
  when: manual
  dependencies:
    - plan

# This job destroys the entire infrastructure.
# It is manual and should be run with extreme caution.
destroy:
  stage: destroy
  extends: .aws_setup
  script:
    - terraform destroy --auto-approve
  when: manual
  dependencies:
    - setup